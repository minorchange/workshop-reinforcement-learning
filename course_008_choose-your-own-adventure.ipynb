{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose your own adventure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may or may not want to ask yourself the following questions to give your journey purpose :)\n",
    "- Do you want to work on a predefined gym environment?\n",
    "    - Choose one or more from [https://gym.openai.com/envs/](https://gym.openai.com/envs/) and get familiar with it. Write your own first model and see how far you come. Try different variations. Try to understand why things work or don't work.\n",
    "- Do you want to work on your custom environment?\n",
    "    - Think about a scenario that interests you. Try to phrase it in a gym conform way (see course_002).\n",
    "- Do you want to focus on performance optimization?\n",
    "    - Get a feeling for your problem by manually tweaking parameters.\n",
    "    - Have a look at [optuna](https://pypi.org/project/optuna/). It can help you to automatize hyperparameter tuning. \n",
    "    - How much are you able to improve over your initial model in terms of final performance as well as in terms of the [learning curve](https://en.wikipedia.org/wiki/Learning_curve_(machine_learning))\n",
    "- Are you interested in the process of learning itself?\n",
    "    - Familiarize yourself with the [tensorboard](https://www.tensorflow.org/tensorboard) to watch the algos learn.\n",
    "    - Checkout [callbacks](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html) for stable baselines. Can you think of a way to detect a dead end in training automatically via callbacks? Can you think of a possibility to e.g. adapt the learning rate in a meaningful way? (look [here](https://www.reddit.com/r/reinforcementlearning/comments/sbisjn/change_learning_rate_of_a_saved_model/) for a hint on how to change the learning rate for a loaded model)\n",
    "- Do you want to bring computations on the GPU?\n",
    "    - What are your expectations? Would you expect a similar speedup as in the case of training a deep neural network in a supervised learning setup?\n",
    "- You want to see how a fully trained model will perform on your environment of choice? \n",
    "    - Have a look at pre-trained models on [huggingface](https://huggingface.co/models?other=stable-baselines3).\n",
    "- Want to read more about the algos?\n",
    "    - [Intro to Proximal Policy Optimization (PPO)](https://medium.com/intro-to-artificial-intelligence/proximal-policy-optimization-ppo-a-policy-based-reinforcement-learning-algorithm-3cf126a7562d)\n",
    "    - [Start of a series on RL on towardsdatascience](https://towardsdatascience.com/reinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060)\n",
    "- Want to play with the architecture of the underlying Neural Network? [Custom Architecture](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html); [Intro to Actor Critic Model](https://towardsdatascience.com/proximal-policy-optimization-tutorial-part-1-actor-critic-method-d53f9afffbf6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## More Interesting RL Projects/Resources:\n",
    "\n",
    "- https://spinningup.openai.com/en/latest/  This is an educational resource produced by OpenAI that makes it easier to learn about deep reinforcement learning (deep RL).\n",
    "- https://mujoco.org/ for building simmulations\n",
    "- https://carla.org/ for simmulating traffic - made for e.g. playing with rl for self driving cars\n",
    "- https://gym.openai.com/ creating and handling environments for rl\n",
    "- https://stable-baselines3.readthedocs.io/en/master/ a set of reliable implementations of reinforcement learning algorithms in PyTorch\n",
    "- https://pypi.org/project/optuna/ hyperparameter optimization framework that supports stable-baselines\n",
    "- [https://github.com/DLR-RM/rl-baselines3-zoo](https://github.com/DLR-RM/rl-baselines3-zoo) builds upon SB3, containing optimal hyperparameters for Gym environments as well as code to easily find new ones. Such tuning is almost always required.\n",
    "- [https://github.com/cpnota/autonomous-learning-library](https://github.com/cpnota/autonomous-learning-library) and [https://github.com/thu-ml/tianshou](https://github.com/thu-ml/tianshou) are two reinforcement learning libraries I like that are generally geared towards more experienced users.\n",
    "- [https://github.com/Farama-Foundation/PettingZoo](https://github.com/Farama-Foundation/PettingZoo) is like Gym, but for environments with multiple agents."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d17b2e7b120c55232730544c4730fa6bc7b732aa281f1290f107952489e39c0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
